import requests
import json
import os
import sys
import traceback
import argparse
from typing import Dict, List, Tuple
from pathlib import Path
from datetime import datetime
from openai import OpenAI, AsyncOpenAI
from common.secret import LLM_API_KEY_TRAINER, LLM_API_URL, LLM_MODEL, MY_API_URL, SIMILARITY_THRESHOLD
from dataset.dataset_manager import LeetcodeDataset
import asyncio
from common.shared_utils import run_test, format_problem_prompt, extract_system_prompt, text_similarity_comparison, one_on_one_comparison
from common.shared_utils import generate_test_cases, save_test_cases, save_to_json_file

class NeuronEvaluator:
    """Evaluates code generated by neurons against test cases"""
    
    def __init__(self, api_url: str = MY_API_URL):
        self.api_url = api_url
        self.test_cases = []
        self.neuron_scores = {}  # Format: {(layer_idx, neuron_idx): score}
        self.test_results = {}   # Format: {(layer_idx, neuron_idx): {test_idx: passed}}
        self.training_history = []  # Track scores over training iterations
        self.current_problem = None
        self.openai_client = OpenAI(api_key=LLM_API_KEY_TRAINER, base_url=LLM_API_URL)
        
    def initialize_model(self, user_init: bool = False, model_goal: str = "coding", 
                         parameters: Dict = None, layer_sizes: List[int] = None) -> bool:
        """Initialize the neural network model"""
        init_data = {
            "user_init": user_init,
            "model_goal": model_goal
        }
        
        if user_init and parameters:
            init_data["parameters"] = parameters
        elif not user_init and layer_sizes:
            init_data["layer_sizes"] = layer_sizes
        else:
            print("Error: Must provide parameters for user_init=True or layer_sizes for user_init=False")
            return False
            
        try:
            response = requests.post(f"{self.api_url}/init", json=init_data)
            if response.status_code != 200:
                print(f"Error initializing model: {response.text}")
                return False
            print("Model initialized successfully")
            return True
        except Exception as e:
            print(f"Exception initializing model: {str(e)}")
            traceback.print_exc()
            return False
            
    def save_model(self, output_file: str) -> bool:
        """Save current model parameters to a file"""
        try:
            response = requests.get(f"{self.api_url}/params")
            if response.status_code != 200:
                print(f"Error getting model parameters: {response.text}")
                return False
                
            parameters = response.json()
            return save_to_json_file(parameters, output_file)
        except Exception as e:
            print(f"Error saving model: {str(e)}")
            traceback.print_exc()
            return False
    
    def load_test_cases(self, filepath: str) -> bool:
        """Load test cases from a JSON file"""
        try:
            with open(filepath, 'r') as file:
                data = json.load(file)
                if "test_cases" not in data:
                    print("Invalid test cases file: missing 'test_cases' key")
                    return False
                
                self.test_cases = data["test_cases"]
                
                # If problem is included in the file, set it as current problem
                if "problem" in data:
                    self.current_problem = data["problem"]
                    print(f"Loaded problem description from test cases file")
                    
                print(f"Loaded {len(self.test_cases)} test cases from {filepath}")
                return True
        except Exception as e:
            print(f"Error loading test cases: {str(e)}")
            traceback.print_exc()
            return False
    
    def evaluate_neurons(self) -> Dict[Tuple[int, int], float]:
        """Evaluate all neuron outputs against test cases"""
        if not self.last_run_results or not self.test_cases:
            print("No run results or test cases available")
            return {}
            
        self.neuron_scores = {}
        self.test_results = {}
        evaluation_summary = {
            "problem": self.current_problem,
            "timestamp": datetime.now().isoformat(),
            "scores": {}
        }
        
        for output in self.last_run_results.get("neuron_outputs", []):
            if not output.get("parsed", False):
                continue
                
            layer_idx = output["layer_idx"]
            neuron_idx = output["neuron_idx"]
            language = output.get("language", "").lower()
            code = output["code"]
            
            neuron_key = (layer_idx, neuron_idx)
            
            if language != "python":
                print(f"Neuron ({layer_idx}, {neuron_idx}) used non-Python language: {language}. Setting score to 0.")
                self.neuron_scores[neuron_key] = 0
                evaluation_summary["scores"][f"{layer_idx}_{neuron_idx}"] = 0
                self.test_results[neuron_key] = {}
                continue
            
            try:
                response = requests.get(f"{self.api_url}/extract-code/{layer_idx}/{neuron_idx}")
                if response.status_code == 200:
                    extracted_data = response.json()
                    if "code" in extracted_data:
                        code = extracted_data["code"]
                        print(f"Using extracted code for neuron ({layer_idx}, {neuron_idx})")
            except Exception as e:
                print(f"Error getting extracted code from API: {str(e)}")
                # Continue with the code we already have
            
            # Evaluate this neuron's code against all test cases
            self.test_results[neuron_key] = {}
            passed_tests = 0
            total_tests = len(self.test_cases)
            error_count = 0
            
            for test_idx, test_case in enumerate(self.test_cases):
                try:
                    result = run_test(code, test_case["input"], test_case["expected"], text_similarity_comparison)
                    self.test_results[neuron_key][test_idx] = result
                    if result >= SIMILARITY_THRESHOLD:
                        passed_tests += 1
                except Exception as e:
                    error_count += 1
                    print(f"Error running test {test_idx} for neuron ({layer_idx}, {neuron_idx}): {str(e)}")
            
            # Calculate score as percentage of tests passed
            if total_tests > 0:
                score = passed_tests / total_tests
                self.neuron_scores[neuron_key] = score
                evaluation_summary["scores"][f"{layer_idx}_{neuron_idx}"] = score
                print(f"Neuron ({layer_idx}, {neuron_idx}) score: {score:.2f} ({passed_tests}/{total_tests} tests passed, {error_count} errors)")
            else:
                print("No tests available for evaluation")
                self.neuron_scores[neuron_key] = 0
                
        # Add to training history
        self.training_history.append(evaluation_summary)
        return self.neuron_scores
    
    def save_training_history(self, filepath: str) -> bool:
        """Save training history to a file"""
        return save_to_json_file({"history": self.training_history}, filepath)
    
    def _refine_weights(self, neuron: Dict, layer_idx: int, neuron_idx: int, score: float) -> None:
        """Refine weights based on neuron performance"""
        # Get scores from previous layer neurons that feed into this one
        prev_layer_scores = []
        for prev_neuron_idx in range(len(neuron["w"])):
            prev_key = (layer_idx - 1, prev_neuron_idx)
            if prev_key in self.neuron_scores:
                prev_layer_scores.append((prev_neuron_idx, self.neuron_scores[prev_key]))
        
        if not prev_layer_scores:
            return
            
        # Sort by performance
        prev_layer_scores.sort(key=lambda x: x[1], reverse=True)
        
        # Adjust weights to favor better performing neurons
        total_score = sum(s for _, s in prev_layer_scores)
        if total_score == 0:
            return
            
        # Calculate new weights based on relative performance
        new_weights = []
        for prev_neuron_idx, prev_score in prev_layer_scores:
            weight_idx = prev_neuron_idx
            if weight_idx < len(neuron["w"]):
                # Adjust weight based on performance ratio
                if total_score > 0:
                    relative_score = prev_score / total_score
                    # Keep some randomness for exploration
                    new_weight = 0.7 * relative_score + 0.3 * neuron["w"][weight_idx]
                    new_weights.append((weight_idx, max(0.1, min(0.9, new_weight))))
        
        # Apply new weights
        for idx, weight in new_weights:
            neuron["w"][idx] = weight

    def load_model_from_file(self, param_file: str) -> bool:
        """Load model parameters from a file"""
        try:
            if not os.path.exists(param_file):
                print(f"Parameter file not found: {param_file}")
                return False
                
            with open(param_file, 'r') as f:
                parameters = json.load(f)
                
            return self.initialize_model(
                user_init=True,
                model_goal=parameters.get("model_goal", "coding"),
                parameters=parameters
            )
        except Exception as e:
            print(f"Error loading model parameters: {str(e)}")
            traceback.print_exc()
            return False

    async def async_generate_and_save_test_cases(self, prompt: str, filepath: str, num_tests: int = 20) -> bool:
        """Async version of generate_and_save_test_cases"""
        print(f"Generating and saving {num_tests} test cases for the problem...")
        try:
            # Create a fresh AsyncOpenAI client for this request
            async_client = AsyncOpenAI(
                api_key=LLM_API_KEY_TRAINER,
                base_url=LLM_API_URL
            )
            
            # Generate test cases
            success, test_cases = await generate_test_cases(
                prompt, 
                async_client, 
                LLM_MODEL, 
                num_tests
            )
            
            # Make sure to close the client
            await async_client.close()
            
            if success and test_cases:
                self.test_cases = test_cases
                
                # Save test cases to file
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(
                    None,
                    lambda: save_test_cases(prompt, self.test_cases, filepath)
                )
                
                return True
            
            return False
        except Exception as e:
            print(f"Error in async_generate_and_save_test_cases: {str(e)}")
            traceback.print_exc()
            return False
            
    async def async_run_problem(self, prompt: str) -> bool:
        """Async version of run_problem"""
        try:
            self.current_problem = prompt
            
            # Format the prompt to ensure proper code generation
            formatted_prompt = format_problem_prompt(prompt)
            
            # Use the API endpoint to test neurons with a timeout
            print(f"Sending request to API server with timeout...")
            
            # Use requests in a separate thread to not block the event loop
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: requests.post(
                    f"{self.api_url}/test-neurons", 
                    json={"prompt": formatted_prompt},
                    timeout=360  # 6 minute timeout
                )
            )
            
            if response.status_code != 200:
                print(f"Error running problem: {response.text}")
                # Check for timeout status
                if response.status_code == 504:
                    print("API server returned a timeout response")
                return False
                
            self.last_run_results = response.json()
            
            # Check if the response indicates an error
            if "error" in self.last_run_results:
                print(f"API returned an error: {self.last_run_results['error']}")
                return False
                
            return True
        except requests.exceptions.Timeout:
            print(f"Request timed out after 6 minutes")
            return False
        except Exception as e:
            print(f"Exception running problem: {str(e)}")
            traceback.print_exc()
            return False

    async def async_refine_system_prompt(self, neuron: Dict, layer_idx: int, neuron_idx: int, score: float) -> bool:
        """Async version of refine_system_prompt that uses async OpenAI client"""
        # For low-performing neurons, request a better system prompt
        if score < 0.6:  # Poor performance
            # Get the generated code for this neuron
            code_sample = "# No code found"
            neuron_key = (layer_idx, neuron_idx)
            
            # Try to fetch actual code from the API
            try:
                # Use run_in_executor to make the blocking requests call non-blocking
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: requests.get(f"{self.api_url}/extract-code/{layer_idx}/{neuron_idx}")
                )
                
                if response.status_code == 200:
                    extracted_data = response.json()
                    if "code" in extracted_data:
                        code_sample = extracted_data["code"]
                        print(f"Retrieved code sample for neuron ({layer_idx}, {neuron_idx})")
            except Exception as e:
                print(f"Error getting code sample for neuron ({layer_idx}, {neuron_idx}): {str(e)}")
                # Try to get code from our local results if API fetch failed
                if neuron_key in self.test_results:
                    # Find any code we might have stored
                    for test_run in self.last_run_results.get("neuron_outputs", []):
                        if test_run.get("layer_idx") == layer_idx and test_run.get("neuron_idx") == neuron_idx:
                            if "code" in test_run:
                                code_sample = test_run["code"]
                                break
            
            # Get example test cases that failed
            failed_examples = []
            if neuron_key in self.test_results:
                test_results = self.test_results[neuron_key]
                failed_count = 0
                
                for test_idx, passed in test_results.items():
                    if not passed and failed_count < 3:  # Limit to 3 examples
                        failed_count += 1
                        if test_idx < len(self.test_cases):
                            failed_examples.append(self.test_cases[test_idx])
            
            prompt = f"""
I need to improve a system prompt for a neuron that generates code.

Current performance: {score:.2f} - the code is not passing most test cases.

Current system prompt:
<system_prompt>
{neuron["system_prompt"]}
</system_prompt>

Code generated by this system prompt:
```python
{code_sample}
```

{"Failed test examples:" if failed_examples else ""}
{json.dumps(failed_examples, indent=2) if failed_examples else ""}

Please generate an improved system prompt that will help the model produce more accurate, 
efficient, and correct code. The prompt should emphasize:
1. Careful reading of the problem requirements
2. Proper handling of edge cases
3. Efficient implementation
4. Correct output formatting
5. Thorough testing of the solution
6. Ensuring the code handles input and output properly

Based on the issues observed in the code sample and test failures, provide specific guidance 
to address these weaknesses.

Do not include any other text or explanations in your response except the improved system prompt enclosed in <system_prompt> tags.
"""
            try:
                # Create a fresh AsyncOpenAI client for this request
                async_client = AsyncOpenAI(
                    api_key=LLM_API_KEY_TRAINER,
                    base_url=LLM_API_URL
                )
                
                # Direct OpenAI call for system prompt refinement
                messages = [
                    {"role": "system", "content": "You are a helpful assistant that improves system prompts for code generation."},
                    {"role": "user", "content": prompt}
                ]
                
                # Use AsyncOpenAI client directly
                response = await async_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=messages,
                )
                
                # Make sure to close the client
                await async_client.close()
                
                result = response.choices[0].message.content
                improved_prompt = extract_system_prompt(result)
                if improved_prompt:
                    neuron["system_prompt"] = improved_prompt
                    print(f"Refined system prompt for neuron ({layer_idx}, {neuron_idx})")
                    return True
                return False
            except Exception as e:
                print(f"Error refining system prompt: {str(e)}")
                traceback.print_exc()
                return False
        return False  # No refinement needed

    async def async_refine_model(self) -> Dict:
        """Async version of refine_model that processes neurons concurrently"""
        if not self.neuron_scores:
            print("No evaluation results available")
            return None
            
        try:
            # Get current model parameters
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: requests.get(f"{self.api_url}/params")
            )
            
            if response.status_code != 200:
                print(f"Error getting model parameters: {response.text}")
                return None
                
            current_params = response.json()
            
            # Create a copy of the parameters to refine
            refined_params = current_params.copy()
            
            # Create a list to store all neurons that need prompt refinement
            prompt_refinement_tasks = []
            
            # Group neurons by layer for refinement
            layer_neurons = {}
            for (layer_idx, neuron_idx), score in self.neuron_scores.items():
                if layer_idx not in layer_neurons:
                    layer_neurons[layer_idx] = []
                layer_neurons[layer_idx].append((neuron_idx, score))
            
            # Process each layer and collect neurons that need refinement
            for layer_idx, neurons in layer_neurons.items():
                if layer_idx >= len(refined_params["layers"]):
                    continue
                    
                layer = refined_params["layers"][layer_idx]
                
                for neuron_idx, score in neurons:
                    if neuron_idx >= len(layer["neurons"]):
                        continue
                        
                    neuron = layer["neurons"][neuron_idx]
                    
                    # Refine weights if this is not the first layer
                    if layer_idx > 0:
                        self._refine_weights(neuron, layer_idx, neuron_idx, score)
                    
                    # Queue up system prompt refinement for poor performers
                    if score < 0.6:  # Poor performance threshold
                        prompt_refinement_tasks.append((neuron, layer_idx, neuron_idx, score))
            
            # Process all system prompt refinements concurrently using async_refine_system_prompt
            if prompt_refinement_tasks:
                print(f"Refining system prompts for {len(prompt_refinement_tasks)} neurons concurrently...")
                
                # Use asyncio.gather to process all refinements concurrently
                refinement_results = await asyncio.gather(
                    *[self.async_refine_system_prompt(neuron, layer_idx, neuron_idx, score) 
                      for neuron, layer_idx, neuron_idx, score in prompt_refinement_tasks]
                )
                
                # Print summary of refinements
                success_count = sum(1 for result in refinement_results if result)
                print(f"Successfully refined {success_count} out of {len(prompt_refinement_tasks)} system prompts")
            
            # Update the model with refined parameters
            update_response = await loop.run_in_executor(
                None,
                lambda: requests.post(f"{self.api_url}/params", json=refined_params)
            )
            
            if update_response.status_code != 200:
                print(f"Error updating model parameters: {update_response.text}")
                return None
                
            print("Model parameters refined successfully")
            return refined_params
            
        except Exception as e:
            print(f"Exception refining model: {str(e)}")
            traceback.print_exc()
            return None

async def async_train_on_problems(evaluator: NeuronEvaluator, problems: List[Dict], output_dir: str, num_problems: int = 100, checkpoint_interval: int = 10):
    """Train the model on a set of problems with concurrent operations"""
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_dir = os.path.join(output_dir, f"checkpoint_{timestamp}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    initial_model_path = os.path.join(output_dir, "initial_model.json")
    evaluator.save_model(initial_model_path)
    
    problem_count = min(num_problems, len(problems))
    print(f"Training on {problem_count} problems")
    
    success_count = 0
    failure_count = 0
    
    for i, problem in enumerate(problems[:problem_count], 1):
        print(f"Problem {i}/{problem_count}")
        
        # Extract problem statement
        problem_text = problem.get("description", "")
        if not problem_text:
            print("Skipping problem with no description")
            continue
        
        # Run test case generation and problem execution concurrently
        test_case_path = os.path.join(checkpoint_dir, f"test_cases_{i}.json")
        
        print("Starting concurrent test case generation and problem execution...")
        # Start both tasks concurrently
        test_case_task = asyncio.create_task(evaluator.async_generate_and_save_test_cases(problem_text, test_case_path))
        problem_task = asyncio.create_task(evaluator.async_run_problem(problem_text))
        
        # Wait for both tasks to complete
        test_case_success, problem_success = await asyncio.gather(test_case_task, problem_task)
        print("Completed concurrent operations")
        
        if not test_case_success:
            print(f"Failed to generate and save test cases for problem {i}, skipping")
            failure_count += 1
            continue
            
        if not problem_success:
            print(f"Failed to run problem {i}, skipping")
            failure_count += 1
            continue
            
        # Evaluate neuron outputs
        scores = evaluator.evaluate_neurons()
        if not scores:
            print(f"No neuron scores for problem {i}, skipping")
            failure_count += 1
            continue
            
        # Refine the model using the async version
        refined_params = await evaluator.async_refine_model()
        if not refined_params:
            print(f"Failed to refine model for problem {i}")
            failure_count += 1
        else:
            success_count += 1
            
        # Save checkpoint
        if i % checkpoint_interval == 0 or i == problem_count:
            checkpoint_file = os.path.join(checkpoint_dir, f"model_checkpoint_{i}.json")
            evaluator.save_model(checkpoint_file)
            
            history_file = os.path.join(checkpoint_dir, "training_history.json")
            evaluator.save_training_history(history_file)
            print(f"Saved checkpoint after problem {i}")
            
    print(f"Training completed: {success_count} successful, {failure_count} failed")
    return success_count, failure_count

def train_on_problems(evaluator: NeuronEvaluator, problems: List[Dict], output_dir: str, num_problems: int = 100, checkpoint_interval: int = 10):
    """Wrapper that runs the async training function in an event loop"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(async_train_on_problems(evaluator, problems, output_dir, num_problems, checkpoint_interval))
    finally:
        loop.close()

def main():
    parser = argparse.ArgumentParser(description="Train a neural network on coding problems")
    parser.add_argument("--dataset", "-d", type=str, required=True, help="Path to dataset of coding problems")
    parser.add_argument("--output", "-o", type=str, default="./training_output", help="Output directory for training results")
    parser.add_argument("--problems", "-p", type=int, default=100, help="Number of problems to train on")
    parser.add_argument("--checkpoint", "-c", type=int, default=10, help="Checkpoint interval (save model every N problems)")
    parser.add_argument("--model", "-m", type=str, help="Path to existing model parameters to load")
    parser.add_argument("--layers", "-l", type=str, default="2,1", help="Layer sizes for new model (comma-separated)")
    
    args = parser.parse_args()
    
    # Load dataset
    dataset = LeetcodeDataset(args.dataset)
    if not dataset.problems:
        print("No problems found in dataset")
        sys.exit(1)
    
    # Create evaluator
    evaluator = NeuronEvaluator()
    
    # Initialize model
    if args.model:
        # Load existing model
        success = evaluator.load_model_from_file(args.model)
    else:
        # Create new model
        layer_sizes = [int(size) for size in args.layers.split(",")]
        success = evaluator.initialize_model(
            user_init=False,
            model_goal="coding",
            layer_sizes=layer_sizes
        )
        
    if not success:
        print("Failed to initialize model")
        sys.exit(1)
    
    # Get random problems
    problems = dataset.get_random_problems(args.problems)
    
    # Train on problems
    train_on_problems(
        evaluator, 
        problems, 
        args.output,
        num_problems=args.problems,
        checkpoint_interval=args.checkpoint
    )
    
    # Save final model
    final_model_path = os.path.join(args.output, "final_model.json")
    evaluator.save_model(final_model_path)
    print(f"Final model saved to {final_model_path}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Training interrupted by user")
    except Exception as e:
        print(f"Error in main: {str(e)}")
        traceback.print_exc()
